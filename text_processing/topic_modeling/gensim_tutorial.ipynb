{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Memory independence** – there is no need for the whole training corpus to reside fully in RAM at any one time (can process large, web-scale corpora).\n",
    "- Efficient implementations for several popular vector space algorithms, including **Tf-Idf**, distributed incremental **Latent Semantic Analysis**, distributed incremental **Latent Dirichlet Allocation (LDA)** or **Random Projection**; adding new ones is easy (really!).\n",
    "- I/O wrappers and converters around **several popular data formats**.\n",
    "- **Similarity queries** for documents in their semantic representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core concepts\n",
    "The whole gensim package revolves around the concepts of **corpus**, **vector** and **model**.\n",
    "\n",
    "**Corpus** is collection of digital documents, represented as sparse vectors.\n",
    "\n",
    "**Vector** in the Vector Space Model (VSM), each document is represented by an array of features. \n",
    "**Sparse vector**  is an array in which most of the elements have zero values. To save space, we omit them from the document’s representation.\n",
    "\n",
    "**Model** - for our purposes, a model is a transformation from one document representation to another (or, in other words, from one vector space to another)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corpus of nine documents, each consisting of only a single sentence\n",
    "\n",
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoplist = set('for a of the and to in'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: [u'minors', u'graph', u'system', u'trees', u'eps']...)\n"
     ]
    }
   ],
   "source": [
    "# tokenization the documents: \n",
    "# - removing common words (using a toy stoplist)\n",
    "# - removing words that only once appear in the corpus\n",
    "\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in documents)\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist\n",
    "            if stopword in dictionary.token2id]\n",
    "\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.iteritems() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids) # remove stop words and words that appear only once\n",
    "dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "\n",
    "print (dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert documents to vectors, we’ll use a document representation called **bag-of-words**. In this representation, each document is represented by one vector where each vector element represents a question-answer pair, in the style of:\n",
    "\n",
    ">*“How many times does the word **system** appear in the document? Once.”*\n",
    "\n",
    "It is advantageous to represent the questions only by their (integer) ids. The mapping between the questions and ids is called a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'minors': 0, u'graph': 1, u'system': 2, u'trees': 3, u'eps': 4, u'computer': 5, u'survey': 6, u'user': 7, u'human': 8, u'time': 9, u'interface': 10, u'response': 11}\n"
     ]
    }
   ],
   "source": [
    "# There are twelve distinct words in the processed corpus, which means \n",
    "# each document will be represented by twelve numbers (ie., by a 12-D vector).\n",
    "\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 1), (8, 1), (10, 1)],\n",
       " [(2, 1), (5, 1), (6, 1), (7, 1), (9, 1), (11, 1)],\n",
       " [(2, 1), (4, 1), (7, 1), (10, 1)],\n",
       " [(2, 2), (4, 1), (8, 1)],\n",
       " [(7, 1), (9, 1), (11, 1)],\n",
       " [(3, 1)],\n",
       " [(1, 1), (3, 1)],\n",
       " [(0, 1), (1, 1), (3, 1)],\n",
       " [(0, 1), (1, 1), (6, 1)]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The function doc2bow() simply counts the number of occurences of each distinct word,\n",
    "# converts the word to its integer word id and returns the result as a sparse vector.\n",
    "\n",
    "corpus = [dictionary.doc2bow(line.lower().split()) for line in documents]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation documents from one vector representation into another serves two goals:\n",
    "\n",
    "- To bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and (hopefully) more semantic way.\n",
    "- To make the document representation more compact. This both improves efficiency (new representation consumes less resources) and efficacy (marginal data trends are ignored, noise-reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tf-Idf** is a simple transformation which takes documents represented as bag-of-words counts and applies a weighting which discounts common terms (or, equivalently, promotes rare terms). It therefore converts integer-valued vectors into real-valued ones. It also scales the resulting vector to unit length (in the Euclidean norm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus, normalize=True) # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(5, 0.5773502691896257), (8, 0.5773502691896257), (10, 0.5773502691896257)],\n",
       " [(2, 0.3244870206138555),\n",
       "  (5, 0.44424552527467476),\n",
       "  (6, 0.44424552527467476),\n",
       "  (7, 0.3244870206138555),\n",
       "  (9, 0.44424552527467476),\n",
       "  (11, 0.44424552527467476)],\n",
       " [(2, 0.4170757362022777),\n",
       "  (4, 0.5710059809418182),\n",
       "  (7, 0.4170757362022777),\n",
       "  (10, 0.5710059809418182)],\n",
       " [(2, 0.7184811607083769), (4, 0.49182558987264147), (8, 0.49182558987264147)],\n",
       " [(7, 0.45889394536615247), (9, 0.6282580468670046), (11, 0.6282580468670046)],\n",
       " [(3, 1.0)],\n",
       " [(1, 0.7071067811865475), (3, 0.7071067811865475)],\n",
       " [(0, 0.695546419520037), (1, 0.5080429008916749), (3, 0.5080429008916749)],\n",
       " [(0, 0.6282580468670046), (1, 0.45889394536615247), (6, 0.6282580468670046)]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus] # step 2 -- use the model to transform vectors\n",
    "list(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling ***model[corpus]*** only creates a wrapper around the old corpus document stream – actual conversions are done on-the-fly, during document iteration. We cannot convert the entire corpus at the time of calling ***corpus_transformed = model[corpus]***, because that would mean storing the result in main memory, and that contradicts gensim’s objective of memory-indepedence. If you will be iterating over the transformed ***corpus_transformed*** multiple times, and the transformation is costly, serialize the resulting corpus to disk first and continue using that.\n",
    "\n",
    "\n",
    "Once the transformation model has been initialized, it can be used on any vectors (provided they come from the same vector space, of course), even if they were not used in the training corpus at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.7071067811865476), (8, 0.7071067811865476)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(new_doc.lower().split())\n",
    "vec_tfidf = tfidf[vec_bow] # convert the query to tfidf space \n",
    "vec_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing, LSI (or sometimes LSA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements fast truncated SVD (Singular Value Decomposition). The SVD decomposition can be updated with new observations at any time, for an online, incremental, memory-efficient training.\n",
    "\n",
    "It transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into a latent space of a lower dimensionality. For the toy corpus above we used only 2 latent dimensions, but on real corpora, target dimensionality of 200–500 is recommended as a “golden standard”\n",
    "\n",
    "LSI training is unique in that we can continue “training” at any point, simply by providing more training documents. This is done by incremental updates to the underlying model, in a process called online training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, -0.65946640597973938), (1, -0.14211544403729859)],\n",
       " [(0, -2.0245430433828746), (1, 0.42088758246302371)],\n",
       " [(0, -1.5465535813286542), (1, -0.32358919425711979)],\n",
       " [(0, -1.8111412473028827), (1, -0.58905249699324758)],\n",
       " [(0, -0.93367380356343443), (1, 0.27138940499375308)],\n",
       " [(0, -0.012746183038294626), (1, 0.49016179245310371)],\n",
       " [(0, -0.048882032060470634), (1, 1.1129470269929547)],\n",
       " [(0, -0.080638360994106414), (1, 1.563455946344265)],\n",
       " [(0, -0.27381003921275676), (1, 1.34694158495377)]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first 5 documents are more strongly related to the 1st topic \n",
    "# while the remaining 4 documents to the 2nd topic\n",
    "\n",
    "corpus_lsi = lsi[corpus]\n",
    "list(corpus_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.4618210045327158), (1, -0.07002766527899984)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# new_doc related to 1st topic\n",
    "vec_lsi = lsi[vec_bow] # convert the query to LSI space \n",
    "vec_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'-0.644*\"system\" + -0.404*\"user\" + -0.301*\"eps\" + -0.265*\"response\" + -0.265*\"time\" + -0.240*\"computer\" + -0.221*\"human\" + -0.206*\"survey\" + -0.198*\"interface\" + -0.036*\"graph\"'),\n",
       " (1,\n",
       "  u'0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\" + -0.141*\"eps\" + -0.113*\"human\" + 0.107*\"time\" + 0.107*\"response\" + -0.072*\"interface\"')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(num_topics=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that according to LSI, “graph”, “trees” and “minors” are all related words (and contribute the most to the direction of the 2nd topic), while the 1st topic practically concerns itself with all the other words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projections, RP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It aim to reduce vector space dimensionality. This is a very efficient (both memory- and CPU-friendly) approach to approximating **TfIdf** distances between documents, by throwing in a little randomness. Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a double wrapper over the original corpus: bow->tfidf->rp\n",
    "rp = models.rpmodel.RpModel(corpus_tfidf, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.40824827551841736), (1, 0.40824827551841736)],\n",
       " [(0, 1.0871520042419434), (1, -0.1693640947341919)],\n",
       " [(0, 1.3973586559295654), (1, 0.5898341536521912)],\n",
       " [(0, 0.5080429315567017), (1, 0.5080429315567017)],\n",
       " [(0, 0.32448703050613403), (1, 0.32448700070381165)],\n",
       " [(0, 0.7071067690849304), (1, 0.7071067690849304)],\n",
       " [(1, 1.0)],\n",
       " [(0, 0.4918256103992462), (1, 0.226655513048172)],\n",
       " [(0, 0.5640040636062622), (1, -0.5640040636062622)]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_rp = rp[corpus_tfidf]\n",
    "list(corpus_rp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation, LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is yet another transformation from bag-of-words counts into a topic space of lower dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA), so LDA’s topics can be interpreted as probability distributions over words. These distributions are, just like with LSA, inferred automatically from a training corpus. Documents are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0.77106515881296822), (1, 0.22893484118703167)],\n",
       " [(0, 0.74849867304679052), (1, 0.25150132695320948)],\n",
       " [(0, 0.73472279302826116), (1, 0.26527720697173873)],\n",
       " [(0, 0.71032195545375709), (1, 0.28967804454624291)],\n",
       " [(0, 0.53307688047053547), (1, 0.46692311952946447)],\n",
       " [(0, 0.46485337784009173), (1, 0.53514662215990827)],\n",
       " [(0, 0.27635491841703491), (1, 0.72364508158296503)],\n",
       " [(0, 0.24001017633815902), (1, 0.75998982366184087)],\n",
       " [(0, 0.25321418906769572), (1, 0.74678581093230423)]]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lda = lda[corpus_tfidf]\n",
    "list(corpus_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.154*graph + 0.143*system + 0.128*trees + 0.114*minors + 0.098*eps + 0.075*human + 0.069*survey + 0.060*user + 0.050*interface + 0.038*response'),\n",
       " (1,\n",
       "  u'0.134*user + 0.111*time + 0.110*computer + 0.108*response + 0.102*system + 0.096*interface + 0.077*survey + 0.072*human + 0.068*trees + 0.049*eps')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.show_topics(num_topics=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common reason of semantic analysis is that we want to determine similarity between pairs of documents, or the similarity between a specific document and a set of other documents (such as a user query vs. indexed documents).\n",
    "\n",
    "We will be considering cosine similarity to determine the similarity of two vectors. Cosine similarity is a standard measure in Vector Space Modeling, but wherever the vectors represent probability distributions, different similarity measures may be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pairwise distance matrix\n",
    "index_tfidf = similarities.MatrixSimilarity(corpus_tfidf) # transform corpus to tfidf space and index it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.256485</td>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.283956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.256485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.270671</td>\n",
       "      <td>0.233138</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.329670</td>\n",
       "      <td>0.270671</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>0.191394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283956</td>\n",
       "      <td>0.233138</td>\n",
       "      <td>0.580496</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.191394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.508043</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.718481</td>\n",
       "      <td>0.324487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.508043</td>\n",
       "      <td>0.718481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.670120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.279101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.324487</td>\n",
       "      <td>0.670120</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.000000  0.256485  0.329670  0.283956  0.000000  0.000000  0.000000   \n",
       "1  0.256485  1.000000  0.270671  0.233138  0.707107  0.000000  0.000000   \n",
       "2  0.329670  0.270671  1.000000  0.580496  0.191394  0.000000  0.000000   \n",
       "3  0.283956  0.233138  0.580496  1.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.707107  0.191394  0.000000  1.000000  0.000000  0.000000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.707107   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.000000  0.707107  1.000000   \n",
       "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.508043  0.718481   \n",
       "8  0.000000  0.279101  0.000000  0.000000  0.000000  0.000000  0.324487   \n",
       "\n",
       "          7         8  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.279101  \n",
       "2  0.000000  0.000000  \n",
       "3  0.000000  0.000000  \n",
       "4  0.000000  0.000000  \n",
       "5  0.508043  0.000000  \n",
       "6  0.718481  0.324487  \n",
       "7  1.000000  0.670120  \n",
       "8  0.670120  1.000000  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([i for i in index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.81649655), (1, 0.31412902), (2, 0.0), (3, 0.34777319), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims_tfidf = index_tfidf[vec_tfidf] # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims_tfidf))) # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine measure returns similarities in the range <-1, 1> (the greater, the more similar). So it means that ***vec_tfidf*** has a similarity score of 0.816=81.6% with the document number zero from the corpus, with the second document has a similarity score of 31.4% etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.99844527),\n",
       " (0, 0.99809301),\n",
       " (3, 0.9865886),\n",
       " (1, 0.93748635),\n",
       " (4, 0.90755945),\n",
       " (8, 0.050041765),\n",
       " (7, -0.098794639),\n",
       " (6, -0.10639259),\n",
       " (5, -0.12416792)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_lsi = similarities.MatrixSimilarity(corpus_lsi)\n",
    "sims_lsi = index_lsi[vec_lsi]\n",
    "sorted(enumerate(sims_lsi), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0),\n",
       " (1, 0.0),\n",
       " (2, 0.0),\n",
       " (3, 0.0),\n",
       " (4, 0.0),\n",
       " (5, 0.0),\n",
       " (6, 0.0),\n",
       " (7, 0.0),\n",
       " (8, 0.0)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_rp = similarities.MatrixSimilarity(corpus_rp)\n",
    "sims_rp = index_rp[rp[vec_bow]]\n",
    "sorted(enumerate(sims_rp), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9962514),\n",
       " (4, 0.99348116),\n",
       " (1, 0.98535186),\n",
       " (2, 0.96887434),\n",
       " (5, 0.61379099),\n",
       " (6, 0.47172993),\n",
       " (8, 0.43059778),\n",
       " (3, 0.41381741),\n",
       " (7, 0.41209233)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_lda = similarities.MatrixSimilarity(corpus_lda)\n",
    "sims_lda = index_lda[lda[vec_bow]]\n",
    "sorted(enumerate(sims_lda), key=lambda item: -item[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
